\begin{frame}
	\begin{itemize}\justifying
		\item<1-> We will soon see a more principled approach for learning these weights and threshold but before that let us answer this question...
		\item<2-> Apart from implementing boolean functions (which does not look very interesting) what can a perceptron be used for ?
		\item<3-> Our interest lies in the use of perceptron as a binary classifier. Let us see what this means...
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{columns}
		\column{0.4\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}

			\begin{center}
				\onslide<5->{
					\begin{tikzpicture}
						\node (input0) at (7,-0.1)  {$x_{0}=1$};
						\node (input1) at (8,-0.1)  {$x_{1}$};
						\node (input2) at (9,-0.1)  {$x_{2}$};
						\node (input3) at (10,-0.1)  {$..$};
						\node (input4) at (11,-0.1)  {$..$};
						\node (input5) at (12,-0.1)  {$x_{n}$};

						\node [hidden_neuron] (neuron1) at (10,2)  {};


						\node (output0)  at (10,3.5) {$y$};

						\draw [->] (input0) -- (neuron1);
						\draw [->] (input1) -- (neuron1);
						\draw [->] (input2) -- (neuron1);
						\draw [->] (input3) -- (neuron1);
						\draw [->] (input4) -- (neuron1);
						\draw [->] (input5) -- (neuron1);

						\draw [->] (neuron1) -- (output0);

						\onslide<6->{\node (formula)[scale=.8] at (7.2,0.6) {$w_{0} = -\theta$};}
						\onslide<6->{\node (formula)[scale=.8] at (8.4,0.6) {$w_{1}$};}
						\onslide<6->{\node (formula)[scale=.8] at (9.1,0.6) {$w_{2}$};}
						\onslide<6->{\node (formula)[scale=.8] at (9.8,0.6) {$..$};}
						\onslide<6->{\node (formula)[scale=.8] at (10.4,0.6) {$..$};}
						\onslide<6->{\node (formula)[scale=.8] at (11.1,0.6) {$w_{n}$};}
					\end{tikzpicture}
				}
			\end{center}

			\vspace{-0.5in}
			\only<4->{
				\begin{align*}
					x_1       & = isActorDamon                               \\
					x_2       & = isGenreThriller                            \\
					x_3       & = isDirectorNolan                            \\
					x_4       & = imdbRating \textit{(scaled to 0 to 1)}     \\
					... \quad & \quad...                                     \\
					x_n       & = criticsRating  \textit{(scaled to 0 to 1)}
				\end{align*}
			}


		\end{overlayarea}

		\column{0.6\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<1-> Let us reconsider our problem of deciding whether to watch a movie or not
				\item<2-> Suppose we are given a list of $m$ movies and a label (class) associated with each movie indicating whether the user liked this movie or not : binary decision
				\item<3-> Further, suppose we represent each movie with $n$ features (some boolean, some real valued)
				\item<5-> We will assume that the data is linearly separable and we want a perceptron to learn how to make this decision
				\item<6-> In other words, we want the perceptron to find the equation of this separating plane (or find the values of $w_0, w_1, w_2, .., w_m$)
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}


\begin{frame}
	\begin{columns}

		\column{0.55\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}

			\begin{center}
				\begin{algorithm}[H]
					\onslide<2->{$P \leftarrow inputs\quad with \quad label \quad 1$\;}
					\onslide<3->{$N \leftarrow inputs \quad with \quad label \quad 0$\;}
					\onslide<4->{Initialize $\mathbf{w}$ randomly\;}
					\onslide<5->{\While{$!convergence$}{
							\onslide<7->{Pick random $\mathbf{x} \in P \cup N$ \;}
							\onslide<8->{\If{$\mathbf{x} \in P \quad and \quad \sum_{i=1}^{n} w_i * x_i < 0$} {
									\onslide<9->{$\mathbf{w} = \mathbf{w} + \mathbf{x}$ \;}
								}}
							\onslide<10->{\If{$\mathbf{x} \in N \quad and \quad \sum_{i=1}^{n} w_i * x_i \geq 0$} {
									\onslide<11->{$\mathbf{w} = \mathbf{w} - \mathbf{x}$ \;}
								}}
						}}
					\onslide<6->{//the algorithm converges when all the inputs are classified correctly}
					\caption{Perceptron Learning Algorithm}
				\end{algorithm}
			\end{center}

		\end{overlayarea}


		\column{0.45\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				%\item<1-> Let $P$ be the set of all inputs which have a label 1 (watch movie)
				%\item<1-> Let $N$ be the set of all inputs which have a label 0 (do not watch movie)
				\item<12-> Why would this work ?
				\item<13-> To understand why this works we will have to get into a bit of Linear Algebra and a bit of geometry...
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}


\begin{frame}
	\begin{columns}

		\column{0.5\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<1-> Consider two vectors $\mathbf{w}$ and $\mathbf{x}$
				      \only<2->{
					      \begin{align*}
						      \mathbf{w}                     & = [w_0, w_1, w_2, ..., w_n]               \\
						      \mathbf{x}                     & = [1, x_1, x_2, ..., x_n]                 \\
						      \onslide<3->{\mathbf{w\cdot x} & = \mathbf{w^Tx} = \sum_{i=0}^{n} w_i*x_i}
					      \end{align*}
				      }
				\item<4-> We can thus rewrite the perceptron rule as
				      \only<5->{
					      \begin{align*}
						      y & = 1 \quad if \quad \mathbf{w^Tx} \geq 0 \\
						        & = 0 \quad if \quad \mathbf{w^Tx} < 0
					      \end{align*}
				      }
			\end{itemize}

		\end{overlayarea}

		\column{0.5\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<6-> We are interested in finding the line $\mathbf{w^Tx} = 0$ which divides the input space into two halves
				\item<7-> Every point ($\mathbf{x}$) on this line satisfies the equation $\mathbf{w^Tx} = 0$
				\item<8-> What can you tell about the angle ($\alpha$) between $\mathbf{w}$ and any point ($\mathbf{x}$) which lies on this line ?
				\item<9-> The angle is 90$\degree$ ($\because cos \alpha = \frac{w^Tx}{||w||||x||} = 0$)
				\item<10-> Since the vector $\mathbf{w}$ is perpendicular to every point on the line it is actually perpendicular to the line itself
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}

\begin{frame}
	\begin{columns}

		\column{0.6\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<2-> Consider some points (vectors) which lie in the positive half space of this line (\textit{i.e.}, $\mathbf{w^Tx} \geq 0$)
				\item<3-> What will be the angle between any such vector and $\mathbf{w}$ ? \only<4->{Obviously, less than 90$\degree$}
				\item<5-> What about points (vectors) which lie in the negative half space of this line (\textit{i.e.}, $\mathbf{w^Tx} < 0$)
				\item<6-> What will be the angle between any such vector and $\mathbf{w}$ ? \only<7->{Obviously, greater than 90$\degree$}
				\item<7-> Of course, this also follows from the formula ($cos \alpha = \frac{w^Tx}{||w||||x||}$)
				\item<8-> Keeping this picture in mind let us revisit the algorithm

			\end{itemize}

		\end{overlayarea}

		\column{0.4\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{center}
				\begin{tikzpicture}

					\draw[thick,->] (0,0) -- (3.5,0);
					\draw[thick,->] (0,0) -- (0,3.5);

					\onslide<1->{\draw[densely dashed] (-2,2) -- (2,-2);}
					\onslide<1->{\draw[-{>[length=3]}] (0,0) -- (2.2, 2.2);}
					\onslide<3->{\draw[-{>[length=3]}, blue] (0,0) -- (-0.5, 1.2);}
					\onslide<3->{\draw[-{>[length=3]}, blue] (0,0) -- (0.5, 2);}
					\onslide<3->{\draw[-{>[length=3]}, blue] (0,0) -- (3, 1);}
					\onslide<6->{\draw[-{>[length=3]}, red] (0,0) -- (-1.5, 0.5);}
					\onslide<6->{\draw[-{>[length=3]}, red] (0,0) -- (-0.5, -2);}
					\onslide<6->{\draw[-{>[length=3]}, red] (0,0) -- (0.5, -2);}


					\node at (3.3, -0.2) {$x_1$};
					\node at (-0.2, 3.3) {$x_2$};

					\onslide<2->{\node at (-0.5, 1.5) {$p_1$};}
					\onslide<2->{\node at (0.5, 2.3) {$p_2$};}
					\onslide<2->{\node at (3, 1.3) {$p_3$};}
					\onslide<5->{\node at (-1.5, 0.2) {$n_1$};}
					\onslide<5->{\node at (-0.5, -2.3) {$n_2$};}
					\onslide<5->{\node at (0.5, -2.3) {$n_3$};}
					%\node at (-0.1, 2.3) {$(0,1)$};
					%\node at (2.0, -0.3) {$(1,0)$};
					\node at (2.5, 2.5) {$\mathbf{w}$};
					\onslide<1->{\node at (-1, 2) {$\mathbf{w^Tx} = 0$};}

					\onslide<2->{\filldraw[blue] (-0.5,1.2) circle (2pt);}
					\onslide<2->{\filldraw[blue] (0.5,2) circle (2pt);}
					\onslide<2->{\filldraw[blue] (3,1) circle (2pt);}
					\onslide<5->{\filldraw[red] (-1.5,0.5) circle (2pt);}
					\onslide<5->{\filldraw[red] (-0.5,-2) circle (2pt);}
					\onslide<5->{\filldraw[red] (0.5,-2) circle (2pt);}
					%\filldraw (2,0) circle (2pt);
					%\filldraw (2,2) circle (2pt);
				\end{tikzpicture}
			\end{center}

		\end{overlayarea}
	\end{columns}
\end{frame}

\begin{frame}
	\begin{columns}

		\column{0.55\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{center}
				\begin{algorithm}[H]
					$P \leftarrow inputs\quad with \quad label \quad 1$\;
					$N \leftarrow inputs \quad with \quad label \quad 0$\;
					Initialize $\mathbf{w}$ randomly\;
					\While{$!convergence$}{
						Pick random $\mathbf{x} \in P \cup N$ \;
						\If{$\mathbf{x} \in P \quad and \quad \mathbf{w.x} < 0$} {
							$\mathbf{w} = \mathbf{w} + \mathbf{x}$ \;
						}
						\If{$\mathbf{x} \in N \quad and \quad \mathbf{w.x} \geq 0$} {
							$\mathbf{w} = \mathbf{w} - \mathbf{x}$ \;
						}
					}
					//the algorithm converges when all the inputs are classified correctly
					\caption{Perceptron Learning Algorithm}
				\end{algorithm}

				\vspace{-0.2in}

				\scriptsize{
					\begin{align*}
						cos \alpha = \frac{\mathbf{w}^T\mathbf{x}}{||\mathbf{w}||||\mathbf{x}||}
					\end{align*}
				}
			\end{center}

		\end{overlayarea}

		\column{0.45\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<2-> For $\mathbf{x} \in P$ if $\mathbf{w.x} < 0$ then it means that the angle ($\alpha$) between this $\mathbf{x}$ and the current $\mathbf{w}$ is greater than 90$\degree$ \onslide<3->{(but we want $\alpha$ to be less than 90$\degree$)}
				\item<4-> What happens to the new angle ($\alpha_{new}$) when $\mathbf{w_{new}} = \mathbf{w} + \mathbf{x}$
				      \onslide<5-> {
					      \begin{align*}
						      \onslide<6-> {cos (\alpha_{new})  & \propto \mathbf{w_{new}}^{T}\mathbf{x} \\}
						      \onslide<7-> {                    & \propto (\mathbf{w} + \mathbf{x})^T\mathbf{x} \\}
						      \onslide<8-> {                    & \propto \mathbf{w}^T\mathbf{x} + \mathbf{x}^T\mathbf{x} \\}
						      \onslide<9-> {                    & \propto cos \alpha + \mathbf{x}^T\mathbf{x} \\}
						      \onslide<10-> {cos (\alpha_{new}) & > cos \alpha  }
					      \end{align*}
				      }
				      \vspace{-0.35in}
				\item<11-> Thus $\alpha_{new}$ will be less than $\alpha$ and this is exactly what we want
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}

\begin{frame}
	\begin{columns}

		\column{0.55\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{center}
				\begin{algorithm}[H]
					$P \leftarrow inputs\quad with \quad label \quad 1$\;
					$N \leftarrow inputs \quad with \quad label \quad 0$\;
					Initialize $\mathbf{w}$ randomly\;
					\While{$!convergence$}{
						Pick random $\mathbf{x} \in P \cup N$ \;
						\If{$\mathbf{x} \in P \quad and \quad \mathbf{w.x} < 0$} {
							$\mathbf{w} = \mathbf{w} + \mathbf{x}$ \;
						}
						\If{$\mathbf{x} \in N \quad and \quad \mathbf{w.x} \geq 0$} {
							$\mathbf{w} = \mathbf{w} - \mathbf{x}$ \;
						}
					}
					//the algorithm converges when all the inputs are classified correctly
					\caption{Perceptron Learning Algorithm}
				\end{algorithm}
				\vspace{-0.2in}

				\scriptsize{
					\begin{align*}
						cos \alpha = \frac{\mathbf{w}^T\mathbf{x}}{||\mathbf{w}||||\mathbf{x}||}
					\end{align*}
				}

			\end{center}

		\end{overlayarea}

		\column{0.45\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<2-> For $\mathbf{x} \in N$ if $\mathbf{w.x} \geq 0$ then it means that the angle ($\alpha$) between this $\mathbf{x}$ and the current $\mathbf{w}$ is less than 90$\degree$ \onslide<3->{(but we want $\alpha$ to be greater than 90$\degree$)}
				\item<4-> What happens to the new angle ($\alpha_{new}$) when $\mathbf{w_{new}} = \mathbf{w} - \mathbf{x}$
				      \onslide<5-> {
					      \begin{align*}
						      \onslide<6-> {cos (\alpha_{new})  & \propto \mathbf{w_{new}}^{T}\mathbf{x} \\}
						      \onslide<7-> {                    & \propto (\mathbf{w} - \mathbf{x})^T\mathbf{x} \\}
						      \onslide<8-> {                    & \propto \mathbf{w}^T\mathbf{x} - \mathbf{x}^T\mathbf{x} \\}
						      \onslide<9-> {                    & \propto cos \alpha - \mathbf{x}^T\mathbf{x} \\}
						      \onslide<10-> {cos (\alpha_{new}) & < cos \alpha  }
					      \end{align*}
				      }
				      \vspace{-0.35in}
				\item<11-> Thus $\alpha_{new}$ will be greater than $\alpha$ and this is exactly what we want
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}

\begin{frame}
	\begin{itemize}\justifying
		\item<1-> We will now see this algorithm in action for a toy dataset
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{columns}

		\column{0.4\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{center}
				\input{modules/Module5/tikz_pictures/perceptron_params}
			\end{center}
		\end{overlayarea}

		\column{0.6\textwidth}
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}\justifying
				\item<1-> We initialized $\mathbf{w}$ to a random value
				\item<2-> We observe that currently, $\mathbf{w}\cdot \mathbf{x} < 0$ ($\because$ angle $>$ 90$\degree$) for all the positive points and  $ \mathbf{w} \cdot \mathbf{x} \geq 0$ ($\because$ angle $<$ 90$\degree$) for all the negative points (the situation is exactly opposite of what we actually want it to be)
				\item<3-> We now run the algorithm by randomly going over the points
				      \only<4-5>{\item Randomly pick a point (say, $p_1$), apply correction $\mathbf{w} = \mathbf{w} + \mathbf{x}$ $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually) }
				      \only<6-7>{\item Randomly pick a point (say, $p_2$), apply correction $\mathbf{w} = \mathbf{w} + \mathbf{x}$ $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually) }
				      \only<8-9>{\item Randomly pick a point (say, $n_1$), apply correction $\mathbf{w} = \mathbf{w} - \mathbf{x}$ $\because \mathbf{w\cdot x \geq 0}$ (you can check the angle visually) }
				      \only<10-11>{\item Randomly pick a point (say, $n_3$), no correction needed $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually) }
				      \only<12-13>{\item Randomly pick a point (say, $n_2$), no correction needed $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually)}
				      \only<14-15>{\item Randomly pick a point (say, $p_3$), apply correction $\mathbf{w} = \mathbf{w} + \mathbf{x}$ $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually) }
				      \only<16-17>{\item Randomly pick a point (say, $p_1$), no correction needed $\because \mathbf{w\cdot x \geq 0}$ (you can check the angle visually)}
				      \only<18-19>{\item Randomly pick a point (say, $p_2$), no correction needed $\because \mathbf{w\cdot x \geq 0}$ (you can check the angle visually)}
				      \only<20-21>{\item Randomly pick a point (say, $n_1$), no correction needed $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually)}
				      \only<22-23>{\item Randomly pick a point (say, $n_3$), no correction needed $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually)}
				      \only<24-25>{\item Randomly pick a point (say, $n_2$), no correction needed $\because \mathbf{w\cdot x < 0}$ (you can check the angle visually)}
				      \only<26>{\item Randomly pick a point (say, $p_3$), no correction needed $\because \mathbf{w\cdot x \geq 0}$ (you can check the angle visually)}
				\item<27> The algorithm has converged
			\end{itemize}
		\end{overlayarea}
	\end{columns}
\end{frame}

