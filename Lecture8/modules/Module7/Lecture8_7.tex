\begin{frame}
	\myheading{Module 8.7 : Adding Noise to the inputs}
\end{frame}

\begin{frame}
	\vspace{4em}
	\begin{overlayarea}{\textwidth}{\textheight}
		\begin{block}{Other forms of regularization}
			\begin{itemize}
				\item $l_2$ regularization
				\item Dataset augmentation
				\item Parameter Sharing and tying
				\item \textcolor<2->{red}{Adding Noise to the inputs }
				\item Adding Noise to the outputs 
				\item Early stopping
				\item Ensemble methods
				\item Dropout
			\end{itemize}
		\end{block}
	\end{overlayarea}
\end{frame}
			

\begin{frame}
	\begin{columns}
		\column{0.3\textwidth}
							

		\begin{overlayarea}{\textheight}{\textwidth}
			\input{modules/Module7/tikz_images/autoencoder}
		\end{overlayarea}
							
		\column{0.5\textwidth}
							
		\begin{overlayarea}{\textwidth}{\textheight}
			\begin{itemize}
				\justifying
				\item<2-> We saw this in Autoencoder
				\item<3-> We can show that for a simple input output neural network, adding Gaussian noise to the input is equivalent to weight decay ($L_2$ regularisation)
				\item<4-> Can be viewed as data augmentation
			\end{itemize}
									
		\end{overlayarea}
							
	\end{columns}
\end{frame}
% %% page 3
			
			
\begin{frame}
	\Fontvi
	% \lipsum[1]
	\begin{columns}

		\column{0.35\textwidth}
		\vspace{2em}
		\hspace{2em}
		\begin{overlayarea}{\textwidth}{\textheight}
			\input{modules/Module7/tikz_images/tikz2}														
			\[
				\varepsilon \sim \mathcal{N}(0,\,\sigma^{2})    
			\] 
			\vspace{-1em}
			\begin{align*}
				\onslide<2->{\widetilde{x_i} & = x_i + \varepsilon_i                                     \\}
				\onslide<3->{\widehat{y}     & = \sum_{i=1}^{n}w_i x_i                                   \\}
				\onslide<4->{\widetilde{y}   & = \sum_{i=1}^{n}w_i \widetilde{x_i}                       \\}
				\onslide<5->{                & = \sum_{i=1}^{n}w_i x_i + \sum_{i=1}^{n}w_i \varepsilon_i \\}
				\onslide<6->{                & = \widehat{y} + \sum_{i=1}^{n}w_i \varepsilon_i  }        
			\end{align*}
		\end{overlayarea}
													
		\column{0.7\textwidth}
		\vspace{1em}
		\begin{overlayarea}{\textwidth}{\textheight}
			\onslide<7->{\hspace{2em}
				We are interested in $E[(\widetilde{y} - y)^2]$}
			\begin{align*}
				\onslide<8->{E\left[(\widetilde{y} - y)^2\right] & = E\left[\Big(\widehat{y} + \sum_{i=1}^{n}w_i \varepsilon_i - y \Big)^2\right] \\}
				\onslide<9->{                                    & = E\left[\left(  \Big(\widehat{y} - y \Big)+ \Big(\sum_{i=1}^{n}w_i \varepsilon_i \Big) \right)^2\right] \\}
				\onslide<10->{                                   & = E\left[(\widehat{y} - y)^2\right] + E\left[2(\widehat{y}-y) \sum_{i=1}^{n}w_i \varepsilon_i\right] + E\left[ \Big( \sum_{i=1}^{n}w_i \varepsilon_i \Big)^2\right] \\}
				\onslide<11->{                                   & = E\left[(\widehat{y} - y)^2\right] + 0 + E\left[\sum_{i=1}^{n}w_i^2 \varepsilon_i^2\right] \\
				                                                 & \text{($\because  \varepsilon_i$ is independent of $\varepsilon_j$ and $\varepsilon_i$ is independent of ($\widehat{y}$-$y$) )} \\}
				\onslide<12->{                                   & = (E\left[(\widehat{y} - y)^2\right] + \highlight{\sigma^2\sum_{i=1}^{n}w_i^2} \hspace{1em} \text{(same as }L_2 \text{ norm penalty)}}
			\end{align*}
		\end{overlayarea}
	\end{columns}
\end{frame}
